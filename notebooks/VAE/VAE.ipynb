{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077943ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from jax import vmap, random, pmap, grad\n",
    "from jax.lax import pmean\n",
    "\n",
    "from typing import Any, Callable, Sequence, Optional, Union, Dict\n",
    "from flax.training import train_state\n",
    "from flax import struct\n",
    "import optax\n",
    "\n",
    "from functools import partial\n",
    "from flax import jax_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7868c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    encode_fn: Callable = struct.field(pytree_node=False)\n",
    "    decode_fn: Callable = struct.field(pytree_node=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b971b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMLP(nn.Module):\n",
    "    num_layers: int=2\n",
    "    hidden_dim: int=64\n",
    "    output_dim: int=1\n",
    "    activation: Callable=nn.gelu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for _ in range(self.num_layers):\n",
    "            x = nn.Dense(self.hidden_dim)(x)\n",
    "            x = self.activation(x)\n",
    "        mu = nn.Dense(self.output_dim)(x)\n",
    "        logsigma = nn.Dense(self.output_dim)(x)\n",
    "        return mu, logsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    num_layers: int=2\n",
    "    hidden_dim: int=64\n",
    "    output_dim: int=1\n",
    "    activation: Callable=nn.gelu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for _ in range(self.num_layers):\n",
    "            x = nn.Dense(self.hidden_dim)(x)\n",
    "            x = self.activation(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "    \n",
    "class GaussianMLP(nn.Module):\n",
    "    num_layers: int=2\n",
    "    hidden_dim: int=64\n",
    "    output_dim: int=1\n",
    "    activation: Callable=nn.gelu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for _ in range(self.num_layers):\n",
    "            x = nn.Dense(self.hidden_dim)(x)\n",
    "            x = self.activation(x)\n",
    "        mu = nn.Dense(self.output_dim)(x)\n",
    "        logsigma = nn.Dense(self.output_dim)(x)\n",
    "        return mu, logsigma\n",
    "    \n",
    "class MlpEncoder(nn.Module):\n",
    "    latent_dim: int=8\n",
    "    num_layers: int=2\n",
    "    hidden_dim: int=64\n",
    "    activation: Callable=nn.gelu\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, eps):\n",
    "        mu, logsigma = GaussianMLP(self.num_layers, \n",
    "                                   self.hidden_dim,\n",
    "                                   self.latent_dim,\n",
    "                                   self.activation)(x)\n",
    "        z = mu + eps*jnp.sqrt(jnp.exp(logsigma))\n",
    "        kl_loss = 0.5*jnp.sum(jnp.exp(logsigma) + mu**2 - 1.0 - logsigma, axis=-1)\n",
    "        return z, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888ed1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    encoder: nn.Module\n",
    "    decoder: nn.Module\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, eps):\n",
    "        z, _ = self.encoder(x, eps)\n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "\n",
    "    def _encode(self, x, eps):\n",
    "        z, kl_loss = self.encoder(x, eps)\n",
    "        return z, kl_loss\n",
    "\n",
    "    def _decode(self, z):\n",
    "        x = self.decoder(z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder:\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # Define architecture\n",
    "        encoder = GaussianMLP(\n",
    "            num_layers=config.encoder.num_layers,\n",
    "            hidden_dim=config.encoder.hidden_dim,\n",
    "            output_dim=config.encoder.output_dim,\n",
    "            activation=config.encoder.activation,\n",
    "        )\n",
    "        decoder = MLP(\n",
    "            num_layers=config.decoder.num_layers,\n",
    "            hidden_dim=config.decoder.hidden_dim,\n",
    "            output_dim=config.decoder.output_dim,\n",
    "            activation=config.decoder.activation,\n",
    "        )\n",
    "        arch = VAE(encoder, decoder)\n",
    "\n",
    "        # Initialize params\n",
    "        x = jnp.ones(config.input_dim)\n",
    "        eps = jnp.ones(config.eps_dim)\n",
    "        key = random.PRNGKey(config.seed)\n",
    "        params = arch.init(key, x, eps)\n",
    "\n",
    "        # Vectorized functions across a mini-batch\n",
    "        apply_fn = vmap(arch.apply, in_axes=(None,0,0))\n",
    "        encode_fn = vmap(lambda params, x, eps: arch.apply(params, x, eps, method=arch._encode), in_axes=(None,0,0))\n",
    "        decode_fn = vmap(lambda params, z: arch.apply(params, z, method=arch._decode), in_axes=(None,0))\n",
    "\n",
    "        # Optimizer\n",
    "        lr = optax.exponential_decay(\n",
    "            init_value=config.optimizer.learning_rate,\n",
    "            transition_steps=config.optimizer.decay_steps,\n",
    "            decay_rate=config.optimizer.decay_rate\n",
    "        )\n",
    "        tx = optax.adam(\n",
    "            learning_rate=lr, \n",
    "            b1=config.optimizer.beta1, \n",
    "            b2=config.optimizer.beta2,\n",
    "            eps=config.optimizer.eps\n",
    "        )\n",
    "\n",
    "        # Create state\n",
    "        state = TrainState.create(\n",
    "            apply_fn=apply_fn,\n",
    "            params=params,\n",
    "            tx=tx,\n",
    "            encode_fn=encode_fn,\n",
    "            decode_fn=decode_fn\n",
    "        )\n",
    "\n",
    "        # Replicate state across devices\n",
    "        self.state = jax_utils.replicate(state) \n",
    "        self.beta = config.beta\n",
    "\n",
    "    # Computes KL loss across a mini-batch\n",
    "    def kl_loss(self, params, x, eps):\n",
    "        _, loss = self.state.encode_fn(params, x, eps)\n",
    "        return jnp.mean(loss)\n",
    "\n",
    "    # Computes reconstruction loss across a mini-batch for a single MC sample\n",
    "    def recon_loss(self, params, x, eps):\n",
    "        outputs = self.state.apply_fn(params, x, eps)\n",
    "        loss = jnp.mean((x-outputs)**2)\n",
    "        return loss\n",
    "    \n",
    "    # Computes total loss across a mini-batch for multiple MC samples\n",
    "    def loss(self, params, batch):\n",
    "        x, eps = batch\n",
    "        kl_loss = vmap(self.kl_loss, in_axes=(None,None,0))(params, x, eps)\n",
    "        recon_loss = vmap(self.recon_loss, in_axes=(None,None,0))(params, x, eps)\n",
    "        kl_loss = jnp.mean(kl_loss)\n",
    "        recon_loss = jnp.mean(recon_loss)\n",
    "        loss = self.beta*kl_loss + recon_loss\n",
    "        return loss\n",
    "    \n",
    "    @partial(pmap, axis_name='num_devices', static_broadcasted_argnums=(0,))\n",
    "    def eval_losses(self, params, batch):\n",
    "        x, eps = batch\n",
    "        kl_loss = vmap(self.kl_loss, in_axes=(None,None,0))(params, x, eps)\n",
    "        recon_loss = vmap(self.recon_loss, in_axes=(None,None,0))(params, x, eps)\n",
    "        kl_loss = jnp.mean(kl_loss)\n",
    "        recon_loss = jnp.mean(recon_loss)\n",
    "        return kl_loss, recon_loss\n",
    "\n",
    "    # Define a compiled update step\n",
    "    @partial(pmap, axis_name='num_devices', static_broadcasted_argnums=(0,))\n",
    "    def step(self, state, batch):\n",
    "        grads = grad(self.loss)(state.params, batch)\n",
    "        grads = pmean(grads, 'num_devices')\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
